# DocChat - AI-Powered Documentation Assistant

A local development tool that lets you chat with your project documentation using AI and semantic search. Ask questions in natural language and get real-time streaming responses with source citations.

## Features

- ðŸ” **Semantic Search** - Find relevant documentation based on meaning, not just keywords
- ðŸ’¬ **Real-time Streaming** - See responses as they're generated via WebSocket
- ðŸ“š **Source Citations** - Know exactly which files informed each answer
- ðŸ§  **Conversation Memory** - Follow-up questions maintain context
- âš¡ **Smart Incremental Indexing** - MD5 hash tracking only re-indexes new/changed files
- ðŸš€ **GPU-Accelerated Indexing** - Automatic CUDA detection for 6-10x faster embedding generation
- ðŸ”„ **WebSocket Indexing** - Real-time progress updates, no timeout issues on cloud platforms
- ðŸŽ¨ **Clean UI** - Modern React interface with Tailwind CSS 4

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend â”‚
â”‚   (TypeScript)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WebSocket
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server â”‚â”€â”€â”€â”€â”€â†’â”‚    FAISS     â”‚
â”‚    (Python)     â”‚      â”‚ (Vector DB)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude API     â”‚
â”‚  (Anthropic)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Indexing Phase** (via WebSocket)
   - User provides documentation directory path
   - Backend crawls directory for supported file types
   - Calculates MD5 hash for each file to detect changes
   - **Smart incremental indexing:**
     - New files: fully indexed
     - Modified files: old chunks marked deleted, re-indexed
     - Unchanged files: skipped entirely
     - Deleted files: chunks marked as deleted
   - Splits documents into ~1000 token chunks with overlap
   - Generates vector embeddings for semantic search
   - Stores in FAISS with metadata (file path, hash, deleted flag, etc.)
   - Real-time progress updates sent via WebSocket

2. **Query Phase** (via WebSocket)
   - User asks a question via the web UI
   - Query is converted to a vector embedding
   - FAISS finds the 5 most semantically similar chunks (excluding deleted)
   - Chunks are sent as context to Claude API
   - Response streams back in real-time via WebSocket

3. **Smart Updates**
   - File hashes stored in `file_hashes.pkl`
   - MD5 hash comparison detects file changes
   - Only new and modified files are re-indexed
   - Unchanged files skip processing entirely
   - Deleted chunks are soft-deleted (marked, not removed)

## Prerequisites

- Python 3.10+ (3.11 or 3.12 recommended)
- Node.js 18+
- Anthropic API key ([get one here](https://console.anthropic.com/))
- **Windows ARM64 users**: Use WSL2 for best compatibility (see below)

## Windows ARM64 Setup

If you're on Windows ARM64 (Snapdragon/Copilot+ PC), we **strongly recommend using WSL2**:

```bash
# Install WSL2 (PowerShell as Administrator)
wsl --install

# After restart, open Ubuntu and navigate to project
cd /mnt/c/Users/YourUsername/path/to/doc-chat/backend

# Follow the backend setup instructions below
```

**Why WSL2?** Python data science packages (numpy, faiss-cpu, sentence-transformers) have better pre-built wheel support on Linux ARM64 than Windows ARM64. This avoids compilation errors and missing compiler issues.

## Installation

### Backend Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/doc-chat.git
cd doc-chat/backend

# Create virtual environment
python3 -m venv venv

# Activate (Linux/Mac/WSL2)
source venv/bin/activate

# Or activate (Windows CMD - not recommended for ARM64)
venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env and add your ANTHROPIC_API_KEY
```

**requirements.txt:**
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
websockets==12.0
anthropic==0.7.8
python-dotenv==1.0.0
tiktoken>=0.7.0
langchain==0.1.0
langchain-community==0.0.10
faiss-cpu==1.13.2
sentence-transformers==2.2.2
numpy>=1.25.0,<2.0
```

### Frontend Setup

```bash
cd ../frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

## Usage

### 1. Start the Backend

```bash
cd backend
source venv/bin/activate  # or venv\Scripts\activate on Windows
python app.py
```

The API server will start on `http://localhost:8000`

### 2. Start the Frontend

```bash
cd frontend
npm run dev
```

The web UI will be available at `http://localhost:5173`

### 3. Index Your Documentation

On first launch, you'll see the indexing screen. Enter the path to your project documentation:

```
/path/to/your/project/docs
```

Or use relative paths:
```
../my-project
./docs
```

The indexer will process:
- Markdown files (`.md`)
- Text files (`.txt`)
- Code files (`.py`, `.js`, `.ts`, `.tsx`, `.cs`)
- JSON files (`.json`)

### 4. Start Chatting

Example queries to try:

- "How does authentication work?"
- "Explain the payment processing flow"
- "What API endpoints are available for user management?"
- "Show me examples of error handling"
- "What's the database schema for orders?"

## Configuration

### Backend Environment Variables

Create a `.env` file in the `backend/` directory:

```bash
# Required
ANTHROPIC_API_KEY=sk-ant-...

# Optional - General
FAISS_PERSIST_DIR=./data/faiss_db
MAX_CHUNKS_PER_FILE=50
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Optional - Embedding Performance (see Performance Optimization section)
EMBEDDING_BATCH_SIZE=64        # Batch size for GPU encoding
EMBEDDING_CPU_BATCH_SIZE=32    # Batch size for CPU encoding
EMBEDDING_MAX_WORKERS=4        # Number of CPU workers for multiprocessing
FILE_IO_WORKERS=8              # Workers for parallel file reading
```

### Customizing File Types

Edit `backend/indexer.py`:

```python
def _should_index_file(self, filepath: Path) -> bool:
    """Check if file should be indexed"""
    extensions = {
        '.md', '.txt', '.py', '.cs', '.js', '.ts', 
        '.tsx', '.json', '.yaml', '.yml'  # Add more here
    }
    return filepath.suffix.lower() in extensions
```

### Adjusting Chunk Size

Smaller chunks = more precise but less context
Larger chunks = more context but less precise

```python
# In indexer.py
self.text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Adjust this
    chunk_overlap=200,    # And this
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

## Tailwind CSS 4 Note

This project uses **Tailwind CSS 4** with the Vite plugin. No `tailwind.config.js` is needed for basic usage.

**src/index.css:**
```css
@import "tailwindcss";
```

**vite.config.ts:**
```typescript
import tailwindcss from '@tailwindcss/vite'

export default defineConfig({
  plugins: [react(), tailwindcss()],
  // ...
})
```

For custom configuration (optional), create `tailwind.config.ts`:
```typescript
import type { Config } from 'tailwindcss'

export default {
  content: ['./index.html', './src/**/*.{js,ts,jsx,tsx}'],
  theme: {
    extend: {
      colors: {
        primary: '#3b82f6',
      },
    },
  },
} satisfies Config
```

## API Reference

### REST Endpoints

#### POST `/api/index`
Index documents from a directory (legacy endpoint).

**Note:** For long-running indexing operations, use the WebSocket endpoint `/ws/index` instead. This endpoint may timeout on cloud platforms with connection time limits (e.g., Azure's 3.5 minute limit).

**Request:**
```json
{
  "directory": "/path/to/docs"
}
```

**Response:**
```json
{
  "success": true,
  "stats": {
    "files": 42,
    "chunks": 387,
    "new": 10,
    "modified": 5,
    "unchanged": 27,
    "deleted": 2
  }
}
```

#### GET `/api/stats`
Get indexing statistics.

**Response:**
```json
{
  "total_chunks": 387,
  "dimension": 384
}
```

### WebSocket Endpoints

#### WS `/ws/index`
Real-time document indexing with progress updates. Recommended for production use to avoid timeout issues on cloud platforms.

**Send:**
```json
{
  "directory": "/path/to/docs"
}
```

**Receive (multiple progress messages):**

Scan start:
```json
{
  "type": "scan_start",
  "data": {
    "directory": "/path/to/docs"
  }
}
```

File processing:
```json
{
  "type": "file_processing",
  "data": {
    "file": "auth.md",
    "status": "new"
  }
}
```

File processed:
```json
{
  "type": "file_processed",
  "data": {
    "file": "auth.md",
    "chunks": 12
  }
}
```

Embedding generation:
```json
{
  "type": "embedding_start",
  "data": {
    "total_chunks": 387,
    "device": "cuda:0",
    "batch_size": 64
  }
}
```

Embedding progress (batched):
```json
{
  "type": "embedding_progress",
  "data": {
    "processed": 128,
    "total": 387,
    "percent": 33
  }
}
```

Final statistics:
```json
{
  "type": "stats",
  "data": {
    "files": 42,
    "chunks": 387,
    "new": 10,
    "modified": 5,
    "unchanged": 27,
    "deleted": 2
  }
}
```

Completion:
```json
{
  "type": "done",
  "data": {}
}
```

Error (non-fatal):
```json
{
  "type": "error",
  "data": {
    "message": "Failed to process file.txt"
  }
}
```

Fatal error:
```json
{
  "type": "fatal_error",
  "data": {
    "message": "Invalid directory path"
  }
}
```

**Other message types:** `file_skipped`, `file_deleted`, `embedding_progress`, `embedding_info`, `embedding_complete`, `saving`, `save_complete`

#### WS `/ws/chat`
Real-time chat with streaming responses.

**Send:**
```json
{
  "query": "How does authentication work?"
}
```

**Receive (multiple messages):**

Sources message:
```json
{
  "type": "sources",
  "data": [
    {
      "file": "auth.md",
      "path": "/docs/auth.md",
      "chunk": 0
    }
  ]
}
```

Content chunks (streamed):
```json
{
  "type": "content",
  "data": "Authentication in this system uses..."
}
```

Completion signal:
```json
{
  "type": "done"
}
```

## Project Structure

```
doc-chat/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                # FastAPI application & WebSocket
â”‚   â”œâ”€â”€ indexer.py            # Document ingestion & chunking
â”‚   â”œâ”€â”€ retriever.py          # Vector search & Claude integration
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ faiss_db/         # Vector database (auto-created)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx           # Main application component
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat.tsx      # Chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageList.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SourcePanel.tsx
â”‚   â”‚   â”‚   â””â”€â”€ IndexStatus.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts  # WebSocket state management
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â””â”€â”€ index.ts      # TypeScript type definitions
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ docs/                      # Your documentation (example)
â””â”€â”€ README.md
```

## Technology Stack

### Backend
- **FastAPI** - Modern Python web framework
- **FAISS** - Vector database for semantic search
- **LangChain** - Text splitting and document processing
- **Anthropic SDK** - Claude API integration
- **Sentence Transformers** - Embedding generation
- **WebSockets** - Real-time streaming communication

### Frontend
- **React 19** - UI framework
- **TypeScript** - Type-safe JavaScript
- **Vite** - Fast build tool and dev server
- **Tailwind CSS 4** - Modern utility-first styling with Vite plugin
- **Native WebSocket API** - Real-time updates

## Development

### Running Tests

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm test
```

### Building for Production

```bash
# Frontend production build
cd frontend
npm run build

# Serve with backend
cd ../backend
uvicorn app:app --host 0.0.0.0 --port 8000
```

### Code Quality

```bash
# Python linting
cd backend
flake8 .
black .

# TypeScript linting
cd frontend
npm run lint
npm run format
```

## Troubleshooting

### "No module named 'chromadb'" or similar import errors
```bash
# Make sure you're in the virtual environment
source venv/bin/activate  # Linux/Mac/WSL2
venv\Scripts\activate     # Windows

# Reinstall dependencies
pip install -r requirements.txt
```

### WebSocket connection fails
- Ensure backend is running on port 8000
- Check CORS settings in `app.py`
- Verify frontend is using correct WebSocket URL (`ws://localhost:8000/ws/chat`)

### Empty search results
- Check that documents were indexed: `GET /api/stats`
- Verify file extensions are supported in `indexer.py`
- Try re-indexing: Delete `data/faiss_db/` and re-index

### Out of memory during indexing
- Reduce embedding batch size: `EMBEDDING_BATCH_SIZE=16` (for GPU) or `EMBEDDING_CPU_BATCH_SIZE=16` (for CPU)
- Reduce chunk size in `indexer.py`
- Process directories in smaller batches
- Increase system RAM or use swap space

### GPU not being used
- Verify CUDA is available: `python -c "import torch; print(torch.cuda.is_available())"`
- Install PyTorch with CUDA support: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
- Check GPU memory: Large batch sizes may exceed VRAM, try `EMBEDDING_BATCH_SIZE=16`

### Claude API errors
- Verify `ANTHROPIC_API_KEY` is set correctly in `.env`
- Check API usage limits in Anthropic console
- Ensure you're using a supported model name (`claude-sonnet-4-20250514`)

### Windows ARM64 compilation errors
If you get errors about missing compilers or "can't find Rust compiler":
- **Use WSL2** (strongly recommended - see setup section above)
- Or install Visual Studio Build Tools with C++ support (not recommended for this project)
- The WSL2 approach avoids all compilation issues

### Tiktoken Rust compiler error
If you see "can't find Rust compiler" for tiktoken:
- Update to `tiktoken>=0.7.0` which has pre-built wheels
- Or use WSL2 where all packages have proper wheels

## Performance Optimization

### Automatic GPU/CPU Optimization

The indexer automatically detects and uses the best available hardware:

- **GPU (CUDA)**: If a CUDA-compatible GPU is available, embeddings are generated on the GPU with ~6-10x speedup
- **CPU Multiprocessing**: On CPU-only systems, embeddings are generated in parallel across multiple cores with ~3-4x speedup

You'll see the device being used in the console output:
```
Loading embedding model...
Using GPU: NVIDIA GeForce RTX 3080
```
or
```
Loading embedding model...
No GPU available, using CPU with multiprocessing
```

### Tuning Performance

Adjust embedding performance via environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `EMBEDDING_BATCH_SIZE` | 64 | Batch size for GPU encoding. Reduce to 16-32 for GPUs with <4GB VRAM |
| `EMBEDDING_CPU_BATCH_SIZE` | 32 | Batch size for CPU encoding |
| `EMBEDDING_MAX_WORKERS` | 4 | Number of CPU processes for multiprocessing |
| `FILE_IO_WORKERS` | 8 | Workers for parallel file reading |

**Performance by dataset size:**

| Chunks | GPU Time | CPU Time (4 cores) |
|--------|----------|-------------------|
| 100 | ~1s | ~3s |
| 1,000 | ~5s | ~20s |
| 10,000 | ~50s | ~200s |

### For Large Documentation Sets (1000+ files)

The indexer handles large datasets efficiently with:

1. **Batched Embedding Generation** - Processes chunks in configurable batches to control memory usage
2. **Progress Callbacks** - Real-time updates during embedding generation via WebSocket
3. **Smart Change Detection** - Only re-indexes new/modified files using MD5 hashes

### Metadata Filtering
```python
# Search only specific file types
results = self.store.search(
    query,
    # Add filtering logic for specific extensions
)
```

## Advanced Features

### Custom Embedding Models

Replace sentence-transformers default model:

```python
# In indexer.py and retriever.py
self.model = SentenceTransformer('all-mpnet-base-v2')  # Larger, more accurate
# or
self.model = SentenceTransformer('paraphrase-MiniLM-L3-v2')  # Smaller, faster
```

### File Watching

Auto-reindex on file changes:

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class DocChangeHandler(FileSystemEventHandler):
    def on_modified(self, event):
        if event.is_directory:
            return
        # Re-index changed file
        indexer.index_file(event.src_path)

observer = Observer()
observer.schedule(handler, path='./docs', recursive=True)
observer.start()
```

## Security Considerations

- **API Keys**: Never commit `.env` files. Use environment variables in production.
- **Input Validation**: The backend validates file paths to prevent directory traversal.
- **Rate Limiting**: Consider adding rate limits to API endpoints for production use.
- **CORS**: Update `allow_origins` in production to specific domains only.
- **Sandboxing**: Consider running indexing in a sandboxed environment for untrusted documents.

## Roadmap

- [ ] Multi-project support (switch between indexed projects)
- [ ] Export conversation history
- [ ] Support for images/diagrams in documentation
- [ ] Advanced filtering (date ranges, file types, directories)
- [ ] API authentication (JWT tokens)
- [ ] Docker deployment configuration
- [ ] Slack/Discord bot integration
- [ ] VS Code extension

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

MIT License - see [LICENSE](LICENSE) file for details

## Acknowledgments

- [Anthropic](https://www.anthropic.com/) for Claude API
- [FAISS](https://github.com/facebookresearch/faiss) for vector similarity search
- [LangChain](https://www.langchain.com/) for document processing utilities
- [FastAPI](https://fastapi.tiangolo.com/) for the excellent web framework
- [Sentence Transformers](https://www.sbert.net/) for embedding generation

## Support

- ðŸ“§ Email: hello@iceninemedia.com
- ðŸ› Issues: [GitHub Issues](https://github.com/john-mckillip/doc-chat/issues)
- ðŸ’¬ Discussions: [GitHub Discussions](https://github.com/john-mckillip/doc-chat/discussions)

## Changelog

### v1.1.0 (2025-01-22)
- **Performance**: Automatic GPU detection and CUDA acceleration for embedding generation (6-10x speedup)
- **Performance**: CPU multiprocessing for embedding generation on systems without GPU (3-4x speedup)
- **Performance**: Batched embedding with configurable batch sizes and real-time progress updates
- **Config**: New environment variables for tuning embedding performance (`EMBEDDING_BATCH_SIZE`, `EMBEDDING_MAX_WORKERS`, etc.)

### v1.0.0 (2025-01-17)
- Initial release
- Basic indexing and chat functionality
- WebSocket streaming support
- Source citation panel
- FAISS vector database integration
- React 19 + TypeScript frontend
- Tailwind CSS 4 styling

---

Made with â¤ï¸ by John McKillip | Ice Nine Media